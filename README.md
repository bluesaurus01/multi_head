# What does the multi-head do?

Abstract <br>
After the introduction of the Vision Transformer, several methods have been developed that utilize multi- head attention for processing images. However, the role of multi-head attention has not been clarified. In this paper, data is analyzed from the perspective of similarity, attention covering range, and image frequency to understand the role of multi-head. In addition, head masking was performed during the test stage to identify head similarity and the importance of each head. An experiment was recently conducted on the test stage using the Swin Transformer and Vision Transformer as models.

<br>
This repository includes report of this project and codes for Part 2 and Part 3.
